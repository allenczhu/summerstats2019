---
title: "Lecture 2 - Probability and Distributions"
author: "Allen Zhu"
date: "2019-06-16"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Probability Theory

### Definitions and Introductions
An ensemble is a set of of probabilities describing a
particular random process. An ensemble is defined by:

* $x$, the value of a random variable
* $A_x=\{a_1, a_2, a_3, ..., a_i\}$
* $P_x=\{p_1, p_2, p_3, ..., p_i\}$

$A_x$ is the sample space. The sample space of an experiment is the set of all possible outcomes of that experiment.

A random variable (RV) describes an event that we have not yet observed, e.g.:

  * the number of heads in 6 flips of a fair coin,
  * the number of spam e-mails we will receive today,
  * whether it rains today.

An *experiment* is a situation involving chance or probability that leads to results called outcomes.	*Probability* is the measure of how likely an event is.	An *outcome* is the result of a single trial of an experiment. An *event* is one or more outcomes of an experiment.	

For example: if your experiment is to roll a fair, 6-sided die, then the possible outcomes are landing on 1, 2, 3, 4, 5, or 6.
One event of this experiment is rolling a 5.
The probability of rolling a 5 is one sixth.

Rolling a 7 is an example of an impossible event. Therefore, the probability is zero. Rolling a number less than 7 is certain to occur. This is an example of a certain event. The probability of a certain event is 1.

All probabilities have to be be positive and add up to 1. The sum of the probabilities of the distinct outcomes within a sample space is 1.

$P(x=a_i) = p_i$, where $p_i≥0$.

$$
\sum_{a_i \in A_x} p(x=a_i) = 1
$$

Example: probability of finding a certain nucleotide (A, T, C, or G) in the human genome.
$$
p_A = 0.28 \\
p_T = 0.28 \\
p_C = 0.22 \\
p_G = 0.22 \\
$$

### Complement of an Event

The complement of an event $x$ is the set of all outcomes in the sample space that are not included in the outcomes of event $x$. 
The complement of event $x$ is represented by $\bar{x}$. As a probability, $\bar{P}(x)$ is also used.

Given the probability of an event, the probability of its complement can be found by subtracting the given probability from 1.
$$
P(\bar{x}) = 1-P(x)
$$

### Mutually Exclusive Events

Two events are mutually exclusive if they cannot occur at the same time (i.e., they have no outcomes in common).

Example: A single 6-sided die is rolled. What is the probability of rolling an odd number or an even number?

#### Addition Rules

Addition Rule 1: When two events, $x$ and $y$, are mutually exclusive, the probability that $x$ or $y$ will occur is the sum of the probability of each event.

$P(x\ or\ y) = P(x \cup y) = P(x) + P(y)$

Additional Rule 2: When two events, A and B, are non-mutually exclusive, the probability that A or B will occur is:

$$
\begin{align}
P(x\ or\ y) = P(x \cup y) &= P(x) + P(y) - P(x\ and\ y) \\&= P(x) + P(y) - P(x \cap y)
\end{align}
$$

### Independent and Dependent Events
Two events, $x$ and $y$, are independent if the fact that $x$ occurs does not affect the probability of $y$ occurring.

Two events are dependent if the outcome or occurrence of the first affects the outcome or occurrence of the second so that the probability is changed.

### Joint Probabilities

* Joint Probability: $P(x, y)$

If $x$ and $y$ are independent, then $P(x,y) = P(x) P(y)$

* Marginal Probability: 
$$
P(x) = \sum_{y} P(x,y)
$$

A table of single-strand frequencies of human genes:

|     |     | 2nd   |       |       |       |      |
|-----|-----|-------|-------|-------|-------|------|
|     |     | A     | C     | G     | T     | Sum  |
| 1st | A   | 0.069 | 0.057 | 0.077 | 0.05  | 0.25 |
|     | C   | 0.078 | 0.077 | 0.035 | 0.071 | 0.26 |
|     | G   | 0.078 | 0.070 | 0.075 | 0.047 | 0.27 |
|     | T   | 0.029 | 0.058 | 0.081 | 0.048 | 0.21 |
|     | Sum | 0.25  | 0.26  | 0.27  | 0.21  |      |

### Conditional Probability
The conditional probability of an event $y$ in relationship to an event $x$ is the probability that event $y$ occurs given that event $x$ has already occurred. The notation for conditional probability is $P(y|x)$ [pronounced as The probability of event $y$ given $x$].

The probability of $x$ given $y$ is calculated as:

$$
P(x|y) = \frac{P(x,y)}{P(y)}
$$

### Probability Rules

* Product Rule (Chain Rule):
$$
P(x,y) = P(x|y)P(y)
$$

* Sum Rule
$$
\begin{aligned}
P(x) &= \sum_{y} P(x,y) \\
&=\sum_{y} P(x|y)P(y)
\end{aligned}
$$

* Bayes' Theorem

Bayes' Theorem tells you how ot switch the order of variables in a conditional probability (how to convert $P(x|y)$ into $P(y|x))$.

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

$$
P(y|x) = \frac{P(x|y)P(y)}{\sum_{y}P(x|y)P(y)}
$$

### Exercise
A rare disease that affects 0.1% of the population. You take a test, which is marketed to be 99% correct,
 i.e., in 99/100 cases test shows positive when user
 actually has disease, and in 99/100 cases test shows
 negative when user actually does not have disease. 
 
 The test gives you a positive result. What is the probability
 that you have the disease given the test result? 

#### Solution
Write down all the probabilities:

* $P(disease) = 0.001$
* $P(test\ positive|disease) = 0.99$
* $P(test\ negative|no\ disease) = 0.99$

We want to find $P(disease|test\ positive)$.

We can use Bayes' rule.

$$
P(disease|test\ positive) = \frac{P(test\ positive|disease)P(disease)}
{P(test\ positive)}
$$

Use the sum rule to obtain $P(test\ positive)$.

$$
\begin{aligned}
P(test\ positive) &=P(test\ positive|disease)P(disease)\\ 
&+P(test\ positive|no\ disease)P(no\ disease)\\
&=(0.99 × 0.001) + (0.01 × 0.999)
=0.01098
\end{aligned}
$$

Plug this into Bayes' theorem.

$$
\begin{aligned}
P(disease|test\ positive) = \frac{0.99 × 0.001}
{0.01098}
=0.09
\end{aligned}
$$

Therefore the actual probability of you having
the disease given the test result is only 9%, so
it is unlikely you are diseased. 

## Random Variables

#### Discrete Random Variables

A **binary random variable** is a RV that can take on only two values, e.g., $0$ or $1$. Suppose $A$ is a binary RV. We may then write $A \in \lbrace 0, 1 \rbrace$, meaning that $A$ can take on the values 0 or 1. 

A simple example of a situation involving a a binary random variable is the flipping of a fair coin one time. Will the outcome of the flip be heads (1) or tails (0)? If we let $A$ denote the unknown outcome of the flip, then $A$ is a binary random variable. 

**Discrete random variables** have a discrete sample space, meaning that they can take on only a finite number of values. Suppose $\chi = \lbrace 1, 2, 3,4 \rbrace$ is our sample space, and $X$ is a random variable that takes values in this sample space, ie $X \in \chi$. Consider, for instance, reaching into a bag with 4 balls, with labels 1, 2, 3 and 4. What is the label on the ball that you select?

$0 \leq \Pr(X) \leq 1$. If $\Pr(X=i)=0$, you never draw ball $i$. Alternatively, if $\Pr(X=i)= 1$, you always draw ball $i$. In all other cases, ($0 < \Pr(X=i) < 1$),  you sometimes draw ball $i$. Additionally, the ball that you draw must have either a 1, 2, 3 or 4 on it, so 
$$
\begin{align*}
\Pr(X=1) + \Pr(X=2) + \Pr(X=3) + \Pr(X=4) & = 1
\end{align*}
$$
More generally, $\sum_{x \in \chi} \Pr(X=x) =1$.

A discrete random variable $X$ takes on values $x_i$ with probability $p_i$,
$i=1, \ldots, n$, where $\sum_{i=1}^{n} p_i = 1$.


#### Continuous Random Variables

In the discrete case, a random variable $X$ could take on only finitely many values. In the continuous case, the random variable may take on infinitely many values. Let's begin with an example, a Uniform Random Variable.

For continuous random variables the function $p(\cdot)$ is called a **probability density function (PDF)**.

If a random variable $X$ can take on any of a continuum of values, say, 
any value between $0$ and $1$, then we cannot define it by listing
values $x_i$ and giving the probability $p_i$ that $X= x_i$; for any
single value $x_i$, $\mbox{Prob}(X = x_i )$ is zero!  Instead we can define
the *cumulative distribution function*:
\[
F(x) \equiv \mbox{Prob}(X < x ) ,
\]
or the *probability density function} (pdf)*:
\[
\rho (x)\,dx \equiv \mbox{Prob}( X \in [ x, x+\,dx ] ) = F(x+\,dx ) - F(x) .
\]
Letting $dx \rightarrow 0$, we find
\[
\rho (x) = F' (x) ,~~~F(x) = \int_{- \infty}^{x} \rho (t)\,dt .
\]
(For a more formal mathematical derivation, find a textbook on probability
or measure theory.  This will suffice for our purposes.)

### Expected Value and Variance

#### Discrete Random Variables

The **expected value** of a discrete random variable $X$ is defined as

\[
E(X) \equiv \langle X \rangle = \sum_{i=1}^m p_i x_i .
\]
This is also sometimes called the mean of the random variable $X$
and denoted as $\mu$.

Example 1:  Roll a fair die and let $X$ be the value that appears.
Then $X$ takes on the values $1$ through $6$, each with probability $1/6$.

\[
E(X) = \frac{1}{6} \cdot 1 + \frac{1}{6} \cdot 2 + \frac{1}{6} \cdot 3 +
       \frac{1}{6} \cdot 4 + \frac{1}{6} \cdot 5 + \frac{1}{6} \cdot 6 =
\frac{7}{2} .
\]

If $X$ is a discrete random variable and $g$ is any function, then
$g(X)$ is a discrete random variable and

\[
E(g(X)) = \sum_{i=1}^{m} p_i g( x_i ) .
\]

Example:  $g(X) = a X + b$, $a$ and $b$ constants.
\begin{eqnarray*}
E(g(X)) & = & \sum_{i=1}^{m} p_i ( a x_i + b ) \\
        & = & a \sum_{i=1}^{m} p_i x_i ~+~ b~~~
              \mbox{(since } \sum_{i=1}^{m} p_i = 1 ) \\
        & = & a \cdot E(X) + b .
\end{eqnarray*}

Example:  $g(X) = X^2$.  Then $E(g(X)) = \sum_{i=1}^{m} p_i x_i^2$.

In Example 1 above,
\[
E( X^2 ) = \frac{1}{6} \cdot 1^2 + \frac{1}{6} \cdot 2^2 +
           \frac{1}{6} \cdot 3^2 + \frac{1}{6} \cdot 4^2 +
           \frac{1}{6} \cdot 5^2 + \frac{1}{6} \cdot 6^2 = \frac{91}{6} .
\]

Let $\mu = E(X)$ denote the expected value of $X$.
The expected value of the *square of the difference* between $X$ 
and $\mu$ is

\begin{eqnarray*}
E( ( X - \mu )^2 ) & = & \sum_{i=1}^{m} p_i ( x_i - \mu )^2 \\
                   & = & \sum_{i=1}^{m} p_i ( x_i^2 - 2 \mu x_i + \mu^2 ) \\
                   & = & \sum_{i=1}^{m} p_i x_i^2 - 2 \mu \sum_{i=1}^{m} 
                         p_i x_i + \mu^2 \\
                   & = & E( X^2 ) - \mu^2 \\
                   & = & E( X^2 ) - (E(X) )^2 .
\end{eqnarray*}
The quantity $E( X^2 ) - ( E(X) )^2$ is called the **variance**
of the random variable $X$ and is denoted var($X$).  The square root
of the variance, $\sigma \equiv \sqrt{ \mbox{var}(X)}$ is called the
**standard deviation**.  In Example 1 above,
\[
\mbox{var}(X) = \frac{91}{6} - \left( \frac{7}{2} \right)^2 = \frac{35}{12} .
\]

Let $X$ and $Y$ be two random variables and let $c_1$ and $c_2$ be 
constants.  Then

\begin{eqnarray*}
\mbox{var}( c_1 X + c_2 Y ) & = & E( ( c_1 X + c_2 Y )^2 ) ~-~ 
                                  ( E( c_1 X + c_2 Y ) )^2 \\
                            & = & E( c_1^2 X^2 + 2 c_1 c_2 XY + c_2^2 Y^2 ) ~-~
                                  ( c_1 E(X) + c_2 E(Y) )^2 \\
                            & = & c_1^2 E( X^2 ) + 2 c_1 c_2 E(XY) + 
                                  c_2^2 E( Y^2 ) ~- \\
                            &   & [ c_1^2 ( E(X) )^2 +
                                  2 c_1 c_2 E(X) E(Y) + c_2^2 ( E(Y) )^2 ] \\
                            & = & c_1^2 \mbox{var}(X) + c_2^2 \mbox{var}(Y) +
                                  2 c_1 c_2 ( E(XY) - E(X)E(Y) ) .
\end{eqnarray*}
The **covariance** of $X$ and $Y$, denoted cov($X,Y$), is the quantity
$E(XY) - E(X)E(Y)$.

Two random variables $X$ and $Y$ are said to be **independent** if
the value of one does not depend on that of the other; that is, if
the probability that $X = x_i$ is the same regardless of the value
of $Y$ and the probability that $Y = y_j$ is the same regardless of 
the value of $X$.  Equivalently, the probability that $X = x_i$ and
$Y = y_j$ is the product of the probability that $X = x_i$
and the probability that $Y = y_j$.

Useful facts about variance:

$Var(cX) = c^2Var(x)$ if $c$ is a constant.

$Var(X+Y) = Var(X) + Var(Y)$ only if $X$ and $Y$ are independent.

Example:  Toss two fair coins.  There are four equally probable outcomes:
HH, HT, TH, TT.  Let $X$ equal $1$ if first coin is heads,
$0$ if first coin is tails.  Let $Y$ equal $1$ if second coin is heads,
$0$ if second coin is tails.  Then $X$ and $Y$ are independent because,
for example,
\[
\mbox{Prob}( X=1 \mbox{ and } Y=0 ) = \frac{1}{4} = 
\frac{1}{2} \cdot \frac{1}{2} = \mbox{Prob}( X=1 ) \cdot \mbox{Prob}( Y=0 ) ,
\]

and similarly, for all other possible values, 
$\mbox{Prob}( X= x_i \mbox{ and } Y= y_j ) = \mbox{Prob}( X= x_i ) \cdot
\mbox{Prob}( Y= y_j )$.
In contrast, if we define $Y$ to be $0$ if outcome is $TT$ and $1$ otherwise,
then $X$ and $Y$ are not independent because 
$\mbox{Prob}(X=1 \mbox{ and }Y=0) = 0$, yet $\mbox{Prob}(X=1) = 1/2$
and $\mbox{Prob}(Y=0) = 1/4$.

#### Continuous Random Variables

The expected value of a continuous random variable $X$ is then defined by
\[
E(X) = \int_{- \infty}^{\infty} x \rho (x)\,dx .
\]
Note that by definition, $\int_{- \infty}^{\infty} \rho (x)\,dx = 1$.
The expected value of $X^2$ is
\[
E( X^2 ) = \int_{- \infty}^{\infty} x^2 \rho (x)\,dx ,
\]
and the variance is again defined as $E( X^2 ) - (E(X) )^2$.

If $X$ and $Y$ are independent random variables, then cov($X,Y)=0$, and
\[
\mbox{var}( c_1 X + c_2 Y ) = c_1^2 \mbox{var}(X) + c_2^2 \mbox{var}(Y) .
\]

## Discrete Probability Distributions

There are several probability distributions that arise again and again. We'll look at a number of these distributions.
It is worth knowing what type of distribution you expect under different circumstances, because if you expect a particular distribution you can determine the mean and variance that you expect to observe.

### Bernoulli Distribution

$P(X = 1) = p\\ P(X = 0) = 1-p$

PMF: $p^x(1-p)^{1-x}$

$\mu = p$
$\sigma^2 = p(1-p)$


### Binomial Distribution

If a single event or trial has two possible outcomes (say Xi can be 0 or 1 with P(Xi=1)=p), the probability of getting k "ones" in n independent trials is given by the binomial distribution.
If n=1, the probability of getting a zero in the trial is P(X=0)=1-p. The probability of getting a one in the trial is P(X=1)=p. These are the only possibilities [P(X=0)+P(X=1)=1].

If n=2, the probability of getting two zeros is P(X=0) = (1-p)2 (getting a zero on the first trial and then independently getting a zero on the second trial), the probability of getting a one is P(X=1) = p(1-p) + (1-p)p = 2 p (1-p), and the probability of getting two ones is P(X=2) = p2. These are the only possibilities [P(X=0)+P(X=1)+P(X=2)=1].

For general n, we use the binomial distribution to determine the probability of k "ones" in n trials:

$X \sim Bin(n,p)$
$mu = np$
$\sigma^2 = np(1-p)$
$P(k) = P(X = k) = {n \choose k} p^k(1-p)^{n-k}$

"n choose k" is the number of ways that you can arrange k ones and n-k zeros in a row. For instance, if you wrote down, in a row, the results of n coin tosses, the number of different ways that you could write down all the outcomes and have exactly k heads is "n choose k".

### Geometric Distribution

If a single event or trial has two possible outcomes (say Xi can be 0 or 1 with P(Xi=1)=p), the probability of having to observe k trials before the first "one" appears is given by the geometic distribution.
The probability that the first "one" would appear on the first trial is p.

The probability that the first "one" appears on the second trial is (1-p)*p, because the first trial had to have been a zero followed by a one.

By generalizing this procedure, the probability that there will be k-1 failures before the first success is:

$$
X \sim  Geometric(p) \rightarrow p(k) = (1-p)^{k-1}p \\
\mu = \frac{1}{p} \\
\sigma^2 = \frac{1-p}{p^2}
$$

This is the geometric distribution.

A geometric distribution has a mean of $1/p$ and a variance of $(1-p)/p^2$.

Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?

Notice though that the variance in this case is nearly 100! This means that the actual year in which the population will go extinct is very hard to predict accurately. We can see this from the distribution:

### Negative Binomial Distribution

This is an extension of the geometric distribution, describing the waiting time until $r$ "ones" have appeared. The probability of the $r$th "one" appearing on the $k$th trial is given by the negative binomial distribution:

$$
X \sim NB(r,p) \rightarrow  p(k) = {k-1 \choose r-1}(1-p)^{k-r}p^r \\
X \sim  Geometric(p) = X \sim  NB(1,p) \\
\mu = \frac{r}{p} \\ 
\sigma^2 = \frac{r(1-p)}{p^2} 
$$

Example: If a predator must capture 10 prey before it can grow large enough to reproduce, what would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?

Notice that again the variance in this case is quite high (~1000) and that the distribution looks quite skewed (=not symmetric). Some predators will reach reproductive age much sooner and some much later than the average:

### Poisson Distribution

The Poisson distribution arises in two important instances.
First, it is an approximation to the binomial distribution when n is large and p is small.

Secondly, the Poisson describes the number of events that will occur in a given time period when the events occur randomly and are independent of one another. Similarly, the Poisson distribution describes the number of events in a given area when the presence or absence of a point is independent of occurrences at other points.

The Poisson distribution looks like:
$$
X \sim P(\lambda) \rightarrow p(k) = e^{-\lambda}\frac{\lambda^k}{k!} \\
\mu = \lambda \\
\sigma^2 = \lambda \\
P(np) \approx  Bin(n,p) \\
$$

A Poisson distribution has the unique property that its variance equals its mean, . When the Poisson is used as an approximation to the binomial, the mean and the variance both equal np.

Example: If there are 3 109 basepairs in the human genome and the mutation rate per generation per basepair is 10-9, what is the mean number of new mutations that a child will have, what is the variance in this number, and what will the distribution look like?

Example: If hummingbirds arrive at a flower at a rate  per hour, how many visits are expected in x hours of observation and what is the variance in this expectation? If significantly more variance is observed than expected, what might this tell you about hummingbird visits?

Example: (From Romano) If bacteria are spread across a plate at an average density of 5000 per square inch, what is the chance of seeing no bacteria in the viewing field of a microscope if this viewing field is 10-4 square inches? What, therefore, is the probability of seeing at least one cell?

### Hypergeometric Distribution
TBD


## Continuous Probability Distributions

$$
F(X) = CDF, f(x) = pdf \\
$$

For a pdf to be valid,
$$
\int_{-\infty}^{\infty} f(x)dx = 1 \\
F(a) = P(X \leq a) = \int_{-\infty}^a f(x)dx \\
P(a \leq X \leq b) = F(b) - F(a) = P(a < X < b) \\
f(x) = \frac{d}{dx} F(x) \\
$$

Expected Values
$$
E[g(x)] = \sum g(x)f(x) = \int_{-\infty}^{\infty} g(x)f(x)dx \\
E[X + Y] = E[X] + E[Y] \\
E[aX + b] = aE[X] + b \\
Var(aX + b) = a^2Var(X) \\
$$

### Uniform Distribution

Example:  Uniform Distribution in $[0,1]$.

\[
F(x) = \left\{ \begin{array}{cl}
                 0 & \mbox{if } x < 0 \\
                 x & \mbox{if } 0 \leq x \leq 1 \\
                 1 & \mbox{if } x > 1 \end{array} \right. ,~~~
\rho (x) = \left\{ \begin{array}{cl}
                 0 & \mbox{if } x < 0 \\
                 1 & \mbox{if } 0 \leq x \leq 1 \\
                 0 & \mbox{if } x > 1 \end{array} \right.
\]

\[
E(X) = \int_{- \infty}^{\infty} x \rho (x)\,dx = \int_{0}^{1} x\,dx = 
\frac{1}{2} 
\]
\[
\mbox{var}(X) = \int_{0}^{1} x^2\,dx - \left( \frac{1}{2} \right)^2 =
\frac{1}{3} - \frac{1}{4} = \frac{1}{12} 
\]


$$
f(x) = c \hspace{5mm} \alpha \leq x \leq \beta \\
f(x) = 0 \hspace{5mm} otherwise \\
c = \frac{1}{\beta - \alpha} \\
$$

NORMAL APPROX. TO BINOMIAL
$$
P(X = i) \approx P(i - 0.5 < Y < i + 0.5) \\\\
$$

### Exponential Distribution

$$
X \sim  Exp(\lambda) \rightarrow f(x) = \lambda e^{-\lambda x},\ x \geq 0 \\
$$
$$
F(X) = 1 - e^{-\lambda x} \\
\mu = \frac{1}{\lambda} \\
\sigma^2 = \frac{1}{\lambda^2} \\
$$
Distribution is memoryless

$$
E(max(\lambda)) = \frac{3}{2\lambda}\\
$$
$$
Min-  Exp(\lambda_1 + \lambda_2)
$$


### Gamma Distribution

$$
X \sim G(\alpha, \lambda) \rightarrow \frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},\ x \geq 0\\
Exp(\lambda) = G(1,\lambda) \\
\mu = \frac{\alpha}{\lambda} \\
\sigma^2 = \frac{\alpha}{\lambda^2} \\
$$

### Normal (Gaussian) Distribution

Paramters:
Mean $\mu$,
Variance $\sigma^2$.

\[
\rho (x) = \frac{1}{\sigma \sqrt{2 \pi}}~\exp \left( - \frac{(x - \mu )^2}
{2 \sigma^2} \right) ,
\]

\[
F(x) = \frac{1}{\sigma \sqrt{2 \pi}}~\int_{- \infty}^{x} \exp \left( -
\frac{(t - \mu )^2}{2 \sigma^2} \right) \,dt
\]

## Appendix

The Central Limit Theorem

Let $X_1 , \ldots , X_N$ be independent identically distributed (iid)
random variables, with mean $\mu$ and variance $\sigma^2$.  
Consider the average value,
$A_N = \frac{1}{N} \sum_{i=1}^{N} X_i$.  According to the **Law of Large
Numbers**, this average approaches the mean $\mu$ as $N \rightarrow \infty$,
with probability $1$.

Example:  If you toss a fair coin many, many times, the fraction of 
heads will approach $\frac{1}{2}$.

The **Central Limit Theorem** states that, for $N$ sufficiently large,
values of the random variable $A_N$ are *normally distributed* about
$\mu$, with variance $\sigma^2 / N$.  The expression for the variance
follows from the rules we derived for variance of sums and products:
\[
\mbox{var}( A_N ) = \frac{1}{N^2} \sum_{i=1}^{N} \mbox{var} ( X_i ) =
\frac{\sigma^2}{N} .
\]

This means that an observed value for $A_N$ is within one standard deviation
($\sigma / \sqrt{N}$) of $\mu$ about $68.3$\% of the time, within two 
standard deviations about $95.4$\% of the time, and within three standard
deviations about $99.7$\% of the time.  If we wish to compute the expected
value of a random variable by taking the average of many different samples,
this gives us an idea of how much confidence we can place in our computed
approximation.  However, it applies only asymptotically as 
$N \rightarrow \infty$.
